len(Doc.structure): 8 Doc.title: None
len(DocSection.structure): 0 DocSection.title: определения, обозначения и сокращения lvl: 2
яп – язык программирования; ide – integrated development environment; по – программное обеспечение; http – hypertext transfer protocol; html – hypertext markup language; url – uniform resource locator; clf – common log format; dbscan – density-based spatial clustering of applications with noise; bert – bidirectional encoder representations from transformers; лог - файл журнала с записями о событиях в хронологическом порядке.

len(DocSection.structure): 0 DocSection.title: введение lvl: 2
с каждым днем количество веб-серверов и их пользователей растет [1]. в связи с этим возникает необходимость в эффективном анализе и обработке данных журналов событий веб-серверов для обеспечения надежности и безопасности веб-сервисов, а также для выявления и предотвращения возможных аномалий. цель данной работы разработка модели классификации событий в журналах веб-серверов на основе методов машинного обучения. задачи данной работы: изучение и анализ существующих алгоритмов кластеризации. разработка алгоритма предварительной обработки данных журналов для повышения эффективности классификации. создание и обучение модели классификации событий. оценка эффективности разработанной модели на реальных данных. объектом исследования является модель для классификации журналов веб серверов. предметом исследования является алгоритм кластеризации и выявления аномалий. практическая значимость работы заключается в том, что разработанная модель классификации событий в журналах веб-серверов способствует автоматизации процессов обнаружения аномалий, что позволяет сократить время на обработку инцидентов.

len(DocSection.structure): 2 DocSection.title: 1 обзор предметной области lvl: 2

  len(DocSection.structure): 4 DocSection.title: 1.1 основные теоретические положения lvl: 3
  
    len(DocSection.structure): 0 DocSection.title: 1.1.1 работа веб-сервера lvl: 4
    веб-сервер — это по, обрабатывающее запросы к веб-страницам через интернет и предоставляющее http-ответы, как правило, вместе с html-страницей, изображением, файлом, медиа-потоком или другими данными. он работает по следующему принципу: получение запроса: когда пользователь вводит url веб-сайта в браузере или переходит по ссылке, браузер отправляет запрос на веб-сервер, на котором хранится сайт. обработка запроса: веб-сервер анализирует полученный запрос, определяя, какую страницу или ресурс (например, изображение или документ) необходимо предоставить. поиск ресурса: веб-сервер ищет запрашиваемый ресурс на своем хранилище. если ресурс требует дополнительной обработки (например, выполнения скриптов на стороне сервера), сервер выполняет необходимые действия для генерации финального содержимого. отправка ответа: после нахождения или генерации запрашиваемого ресурса, веб-сервер формирует ответ, который включает в себя статус (например, успешно или ошибка) и сам ресурс (html-страницу, изображение и т.д.). отображение ресурса: браузер пользователя получает ответ от веб-сервера, обрабатывает его и отображает содержимое страницы или сообщение об ошибке, если доступ к ресурсу не был получен. схема работы сервера изображена на рис. 1. рисунок 1 — схема работы веб-сервера

    len(DocSection.structure): 0 DocSection.title: 1.1.2 запросы к серверу lvl: 4
    в протоколе http предусмотрено большое количество методов с помощью которых можно совершать запросы. но в повседневной практике используется, как правило только три: get, post и head, именно они и встречаются в наборе данных, который использовался для анализа событий журнала веб-сервера. ниже представлено краткое пояснение к запросам к веб-серверу( таблица 1). таблица 1 — запросы к веб-серверу

    len(DocSection.structure): 0 DocSection.title: 1.1.3 коды ответа http lvl: 4
    коды ответа http — это стандартизированные коды, которые веб-серверы используют для сообщения клиентам о результате их запросов. коды ответа подразделяются на несколько классов, каждый из которых имеет свою сферу применения. ниже представлено краткое пояснение к кодам ответа http( таблица 2). таблица 2 — коды ответа http.

    len(DocSection.structure): 0 DocSection.title: 1.1.4 логи сервера apache lvl: 4
    логи сервера apache — это файлы журналов, в которые записывается информация о всех запросах и действиях, происходящих на веб-сервере. эти данные крайне важны для анализа трафика, отладки, мониторинга безопасности и оптимизации работы сервера. основные типы лог-файлов apache: access log (журнал доступа): содержит информацию о каждом запросе к серверу. здесь можно найти данные о времени запроса, ip-адресе клиента, методе http, запрашиваемом url, статусе http ответа, размере ответа и информацию о браузере пользователя (user-agent). error log (журнал ошибок): фиксирует ошибки, возникшие во время работы сервера. это могут быть проблемы с конфигурацией, недоступность файлов, проблемы с сетевыми соединениями и другие системные ошибки. этот журнал является основным инструментом для диагностики проблем на сервере. custom logs (пользовательские журналы): apache позволяет настраивать логи для записи специфической информации в соответствии с потребностями администратора. это может быть полезно для сбора конкретных данных для анализа. однако в данной работе для анализа журналов событий используется только access log. в записи логов сервера apache используется формат common log format (clf)[4] или его расширения, такие как combined log format. пример стандартной записи в access log в формате clf изображён на рис. 2 рисунок 2 — пример записи логов веб-сервера в логах содержится следующая информация: ip адрес устройства с которого совершен запрос; дата и время запроса; url адрес к которому направлен запрос; метод с помощью которого совершен запрос (get, post и т. д); код ответа на совершенный запрос; referer — адрес страницы с которой был совершен переход на текущую; user-agent пользователя — идентификатор устройства и браузера. с помощью логов мы можем выявить множество метрик: на какие страницы чаще всего заходят пользователи; с каких устройств чаше всего заходят пользователи; какие поисковые боты посещают сайт и какие страницы индексируют; наличие на сайте несуществующих страниц, к которым тем не менее обращаются пользователи и, что наиболее важно поисковые боты; наличие на сайте редиректов; наличие на сайте критических ошибок 5xx.


  len(DocSection.structure): 7 DocSection.title: 1.2 кластеризация объектов или кластерный анализ lvl: 3
  
    len(DocSection.structure): 0 DocSection.title: 1.2.1 понятие кластеризации lvl: 4
    кластеризация (или кластерный анализ)[2]  представляет собой процесс группировки набора элементов в группы, называемые кластерами. целью является собрать в одной группе элементы, которые между собой похожи, в то время как элементы, принадлежащие к разным группам, должны максимально отличаться друг от друга. основное различие между кластеризацией и классификацией заключается в том, что при кластеризации заранее не определены конкретные группы, их выявление происходит в ходе выполнения алгоритма. процесс кластерного анализа включает в себя несколько шагов: выборка данных для кластеризации. определение набора переменных, по которым будет проводиться оценка элементов выборки. при необходимости осуществляется нормализация данных. расчет значений меры сходства между элементами. применение специализированного метода кластерного анализа для формирования групп похожих элементов(кластеров). визуализация и интерпретация полученных результатов кластеризации. на этапе анализа результатов может потребоваться доработка используемых метрик и методов кластеризации для достижения наилучших результатов.

    len(DocSection.structure): 0 DocSection.title: 1.2.2 меры расстояний lvl: 4
    чтобы вычислять значения меры сходства между объектами  нужно составить вектор характеристик для каждого объекта — как правило, это набор числовых значений. однако существуют также алгоритмы, работающие с качественными (категорийными) характеристиками. после того, как определён вектор характеристик, можно провести нормализацию, чтобы все компоненты давали одинаковый вклад при расчете «расстояния». в процессе нормализации все значения приводятся к некоторому диапазону, например, или . далее, для каждой пары объектов измеряется «расстояние» между ними — степень похожести. существует множество метрик, чаще всего используются: евклидово расстояние наиболее распространенная функция расстояния. представляет собой геометрическим расстоянием в многомерном пространстве: квадрат евклидова расстояния применяется для придания большего веса более отдаленным друг от друга объектам. это расстояние вычисляется следующим образом: расстояние городских кварталов (манхэттенское расстояние)[9] это расстояние является средним разностей по координатам. в большинстве случаев эта мера расстояния приводит к таким же результатам, как и для обычного расстояния евклида. однако для этой меры влияние отдельных больших разностей (выбросов) уменьшается (т.к. они не возводятся в квадрат). формула для расчета манхэттенского расстояния: расстояние чебышева это расстояние может оказаться полезным, когда нужно определить два объекта как «различные», если они различаются по какой-либо одной координате. расстояние чебышева вычисляется по формуле: степенное расстояние применяется в случае, когда необходимо увеличить или уменьшить вес, относящийся к размерности, для которой соответствующие объекты сильно отличаются. степенное расстояние вычисляется по следующей формуле: где  и  – параметры, определяемые пользователем. параметр  ответственен за постепенное взвешивание разностей по отдельным координатам, параметр  ответственен за прогрессивное взвешивание больших расстояний между объектами. если оба параметра –  и  — равны двум, то это расстояние совпадает с расстоянием евклида. выбор метрики полностью лежит на исследователе, поскольку результаты кластеризации могут существенно отличаться при использовании разных мер.

    len(DocSection.structure): 0 DocSection.title: 1.2.3 классификация алгоритмов lvl: 4
    иерархические и плоские. иерархические алгоритмы (также называемые алгоритмами таксономии) строят не одно разбиение выборки на непересекающиеся кластеры, а систему вложенных разбиений. таким образом на выходе получается дерево кластеров, корнем которого является вся выборка, а листьями — наиболее мелкие кластера. плоские алгоритмы строят одно разбиение объектов на кластеры. четкие и нечеткие. четкие (или непересекающиеся) алгоритмы каждому объекту выборки ставят в соответствие номер кластера, таким образом каждый объект принадлежит только одному кластеру. нечеткие (или пересекающиеся) алгоритмы каждому объекту ставят в соответствие набор вещественных значений, показывающих степень отношения объекта к кластерам. в итоге каждый объект относится к каждому кластеру с некоторой вероятностью.

    len(DocSection.structure): 0 DocSection.title: 1.2.4 объединение кластеров lvl: 4
    в случае использования иерархических алгоритмов необходимо решить, как объединять между собой кластера, как вычислять «расстояния» между ними. существует несколько метрик: одиночная связь (расстояния ближайшего соседа) в этом методе расстояние между двумя кластерами определяется расстоянием между двумя наиболее близкими объектами (ближайшими соседями) в различных кластерах. результирующие кластеры имеют тенденцию объединяться в цепочки. полная связь (расстояние наиболее удаленных соседей) в этом методе расстояния между кластерами определяются наибольшим расстоянием между любыми двумя объектами в различных кластерах (т.е. наиболее удаленными соседями). этот метод обычно работает очень хорошо, когда объекты происходят из отдельных групп. если же кластеры имеют удлиненную форму или их естественный тип является «цепочечным», то этот метод непригоден. невзвешенное попарное среднее в этом методе расстояние между двумя различными кластерами вычисляется как среднее расстояние между всеми парами объектов в них. метод эффективен, когда объекты формируют различные группы, однако он работает одинаково хорошо и в случаях протяженных («цепочечного» типа) кластеров. взвешенное попарное среднее метод идентичен методу невзвешенного попарного среднего, за исключением того, что при вычислениях размер соответствующих кластеров (то есть число объектов, содержащихся в них) используется в качестве весового коэффициента. поэтому данный метод должен быть использован, когда предполагаются неравные размеры кластеров. невзвешенный центроидный метод в этом методе расстояние между двумя кластерами определяется как расстояние между их центрами тяжести. взвешенный центроидный метод (медиана) этот метод идентичен предыдущему, за исключением того, что при вычислениях используются веса для учета разницы между размерами кластеров. поэтому, если имеются или подозреваются значительные отличия в размерах кластеров, этот метод оказывается предпочтительнее предыдущего.

    len(DocSection.structure): 0 DocSection.title: 1.2.5 обзор алгоритмов кластеризации lvl: 4
    алгоритмы иерархической кластеризации[5] среди алгоритмов иерархической кластеризации выделяются два основных типа: восходящие и нисходящие алгоритмы. нисходящие алгоритмы работают по принципу «сверху-вниз»: в начале все объекты помещаются в один кластер, который затем разбивается на все более мелкие кластеры. более распространены восходящие алгоритмы, которые в начале работы помещают каждый объект в отдельный кластер, а затем объединяют кластеры во все более крупные, пока все объекты выборки не будут содержаться в одном кластере. таким образом строится система вложенных разбиений. результаты таких алгоритмов обычно представляют в виде дерева – дендрограммы. классический пример такого дерева – классификация животных и растений. для вычисления расстояний между кластерами чаще все пользуются двумя расстояниями: одиночной связью или полной связью . к недостатку иерархических алгоритмов можно отнести систему полных разбиений, которая может являться излишней в контексте решаемой задачи. алгоритмы квадратичной ошибки[6] задачу кластеризации можно рассматривать как построение оптимального разбиения объектов на группы. при этом оптимальность может быть определена как требование минимизации среднеквадратической ошибки разбиения: где  — «центр масс» кластера j (точка со средними значениями характеристик для данного кластера). алгоритмы квадратичной ошибки относятся к типу плоских алгоритмов. самым распространенным алгоритмом этой категории является метод k-средних. этот алгоритм строит заданное число кластеров, расположенных как можно дальше друг от друга. работа алгоритма делится на несколько этапов: случайно выбрать k точек, являющихся начальными «центрами масс» кластеров. отнести каждый объект к кластеру с ближайшим «центром масс». пересчитать «центры масс» кластеров согласно их текущему составу. если критерий остановки алгоритма не удовлетворен, вернуться к пункту 2. в качестве критерия остановки работы алгоритма обычно выбирают минимальное изменение среднеквадратической ошибки. так же возможно останавливать работу алгоритма, если на шаге 2 не было объектов, переместившихся из кластера в кластер. к недостаткам данного алгоритма можно отнести необходимость задавать количество кластеров для разбиения. нечеткие алгоритмы наиболее популярным алгоритмом нечеткой кластеризации является алгоритм c-средних (c-means). он представляет собой модификацию метода k-средних. шаги работы алгоритма: выбрать начальное нечеткое разбиение  объектов на  кластеров путем выбора матрицы принадлежности  размера . используя матрицу , найти значение критерия нечеткой ошибки: , где  — «центр масс» нечеткого кластера : . перегруппировать объекты с целью уменьшения этого значения критерия нечеткой ошибки. возвращаться в пункт 2 до тех пор, пока изменения матрицы  не станут незначительными. этот алгоритм может не подойти, если заранее неизвестно число кластеров, либо необходимо однозначно отнести каждый объект к одному кластеру. dbscan (density-based spatial clustering of applications with noise)[3] dbscan кластеризует точки, основываясь на их плотности расположения. этот алгоритм способен находить кластеры произвольной формы и хорошо работает с данными, содержащими шум и выбросы. dbscan определяет кластеры как области высокой плотности, разделенные областями низкой плотности, и не требует предварительного определения количества кластеров. этот алгоритм наиболее подходит для данной задачи, поскольку способен находить выбросы, в нашем случае аномалии, также не надо задавать количество кластеров заранее, алгоритм сам определяет их количество по данным. алгоритмы, основанные на теории графов[7] суть таких алгоритмов заключается в том, что выборка объектов представляется в виде графа , вершинам которого соответствуют объекты, а ребра имеют вес, равный «расстоянию» между объектами. достоинством графовых алгоритмов кластеризации являются наглядность, относительная простота реализации и возможность внесения различных усовершенствований, основанные на геометрических соображениях. основными алгоритмам являются алгоритм выделения связных компонент, алгоритм построения минимального покрывающего (остовного) дерева и алгоритм послойной кластеризации. алгоритм выделения связных компонент в алгоритме выделения связных компонент задается входной параметр  и в графе удаляются все ребра, для которых «расстояния» больше . соединенными остаются только наиболее близкие пары объектов. смысл алгоритма заключается в том, чтобы подобрать такое значение , лежащее в диапазон всех «расстояний», при котором граф «развалится» на несколько связных компонент. полученные компоненты и есть кластеры. для подбора параметра  обычно строится гистограмма распределений попарных расстояний. в задачах с хорошо выраженной кластерной структурой данных на гистограмме будет два пика – один соответствует внутрикластерным расстояниям, второй – межкластерным расстояния. параметр  подбирается из зоны минимума между этими пиками. при этом управлять количеством кластеров при помощи порога расстояния довольно затруднительно. алгоритм минимального покрывающего дерева[8][10] алгоритм минимального покрывающего дерева сначала строит на графе минимальное покрывающее дерево, а затем последовательно удаляет ребра с наибольшим весом. на рисунке изображен пример минимального покрывающего дерева, полученного для девяти объектов(рис. 3). рисунок 3 — минимальное покрывающее дерево для девяти объектов путём удаления связи, помеченной , с длиной равной 6 единицам (ребро с максимальным расстоянием), получаем два кластера:  и . второй кластер в дальнейшем может быть разделён ещё на два кластера путём удаления ребра , которое имеет длину, равную  единицам. послойная кластеризация алгоритм послойной кластеризации основан на выделении связных компонент графа на некотором уровне расстояний между объектами (вершинами). уровень расстояния задается порогом расстояния . например, если расстояние между объектами  , то . алгоритм послойной кластеризации формирует последовательность подграфов графа , которые отражают иерархические связи между кластерами: где  — граф на уровне , , – -ый порог расстояния, – количество уровней иерархии, – пустое множество ребер графа, получаемое при , , то есть граф объектов без ограничений на расстояние (длину ребер графа), поскольку . посредством изменения порогов расстояния , где , возможно контролировать глубину иерархии получаемых кластеров. таким образом, алгоритм послойной кластеризации способен создавать как плоское разбиение данных, так и иерархическое.

    len(DocSection.structure): 0 DocSection.title: 1.2.6 сравнение алгоритмов lvl: 4
    в таблице представлена вычислительная сложность алгоритмов(таблица 3) таблица 3 — вычислительная сложность алгоритмов. в таблице представлено сравнение алгоритмов(таблица 4) таблица 4 — сравнительная таблица алгоритмов.

    len(DocSection.structure): 0 DocSection.title: 1.2.7 выводы lvl: 4
    выбор алгоритма кластеризации dbscan для анализа событий в журналах веб-серверов обусловлен рядом его преимуществ перед другими алгоритмами. во-первых, dbscan отлично подходит для выявления аномалий, так как он способен определить и исключить отдаленные точки, которые не принадлежат ни одному кластеру, рассматривая их как выбросы. это особенно ценно в контексте безопасности веб-серверов, где такие аномалии могут указывать на нестандартное или подозрительное поведение. также, dbscan не требует предварительного указания количества кластеров, что делает его гибким в работе с данными, структура которых заранее неизвестна. это позволяет алгоритму адаптироваться к данным различной природы и выявлять скрытые структуры без необходимости их предварительного определения. помимо этого, dbscan устойчив к шуму и способен выделять кластеры произвольной формы, что делает его применимым к разнообразным наборам данных, включая те, которые содержат сложные и пересекающиеся структуры. кроме того, dbscan эффективен в вычислительном плане, особенно когда используется в сочетании со структурами данных индексации, что позволяет сократить время обработки даже больших объемов данных. таким образом, dbscan является мощным инструментом для кластеризации данных журналов веб-серверов, обеспечивая высокое качество обнаружения аномалий и гибкость в работе с различными типами данных.



len(DocSection.structure): 6 DocSection.title: 2 выбор метода решения lvl: 2

  len(DocSection.structure): 0 DocSection.title: 2.1 постановка задачи lvl: 3
  необходимо разработать модель для кластеризации логов веб-серверов с целью выявления аномальных событий. модель должна быть способна обрабатывать и анализировать записи журналов веб-серверов, выделяя из них статистически значимые кластеры, характеризующие типичное поведение системы, а также идентифицировать отклонения от нормы, которые могут свидетельствовать о потенциальных угрозах или нештатных ситуациях.

  len(DocSection.structure): 0 DocSection.title: 2.2 сбор и предобработка данных lvl: 3
  необходимо организовать сбор данных из журналов веб-серверов, их очистку и нормализацию для последующего использования в модели кластеризации. это включает удаление нерелевантной информации, преобразование текстовых данных в числовой формат и обработку пропущенных значений.

  len(DocSection.structure): 0 DocSection.title: 2.3 исследование и выбор метода кластеризации lvl: 3
  также необходимо было провести анализ существующих алгоритмов кластеризации для определения наиболее подходящего под задачи работы. нужно учитывать способность алгоритма обрабатывать большие объемы данных, его устойчивость к шуму и способность выявлять аномалии.

  len(DocSection.structure): 0 DocSection.title: 2.4 разработка модели lvl: 3
  далее нужно разработать модель, которая будет принимать на вход записи журналов, обрабатывать их и формировать кластеры, представляющие типовые сценарии работы веб-сервера. модель должна быть способна адаптироваться к изменениям в данных и обновлять кластеры со временем.

  len(DocSection.structure): 0 DocSection.title: 2.5 выявление аномалий lvl: 3
  необходимо интегрировать механизмы для определения и отслеживания аномальных событий, которые не соответствуют ни одному из существующих кластеров. это включает разработку правил или использование статистических методов для выявления потенциальных угроз.

  len(DocSection.structure): 0 DocSection.title: 2.6 тестирование и валидация модели lvl: 3
  необходимо провести проверку эффективности модели на тестовых и реальных данных, оценка точности кластеризации и способности модели к детектированию аномалий. в силу отсутствия заранее размеченных данных, оценка качества кластеризации и способности модели к детектированию аномалий будет иметь субъективный характер.


len(DocSection.structure): 3 DocSection.title: 3 описание метода решения lvl: 2

  len(DocSection.structure): 0 DocSection.title: 3.1 генерация набора событий журнала веб-сереров lvl: 3
  в процессе разработки модели классификации событий в журналах веб-серверов возникла необходимость в создании набора данных для начальных этапов обучения модели. из-за отсутствия доступа к реальным журналам веб-сервера было принято решение о генерации искусственного набора данных, соответствующего формату clf. для создания искусственных записей журнала была использована библиотека faker, предназначенная для генерации случайных данных в языке программирования python. эта библиотека позволила сформировать реалистичные ip-адреса и url-адреса, имитируя потенциальные запросы пользователей к веб-серверу. поле user-agent было сгенерировано на основе десяти различных заготовленных значений, которые отражали распространенные браузеры и операционные системы. остальные поля журнала, такие как дата и время запроса, статус ответа сервера и размер передаваемого контента, также создавались случайным образом, обеспечивая разнообразие в генерируемом наборе данных. каждая запись в журнале формировалась путем случайного выбора значений для каждого поля, соблюдая структуру и формат clf. однако в ходе последующего тестирования модели выявилось, что использование сгенерированных данных приводит к неудовлетворительным результатам. модель, обученная на искусственных данных, не смогла адекватно справляться с задачей классификации и выявления аномалий при работе с реальными журналами веб-серверов. анализ показал, что отсутствие реалистичных шаблонов поведения и взаимосвязей в сгенерированных данных не позволяло модели корректно обобщать информацию и выявлять значимые аномалии. в связи с этим было принято решение отказаться от использования искусственно сгенерированных данных и перейти к работе с реальными журналами веб-серверов, что позволило значительно улучшить качество и точность классификации модели.

  len(DocSection.structure): 0 DocSection.title: 3.2 предварительная обработка данных lvl: 3
  был проведён отбор необходимых данных из общего множества записей журнала и удаление тех полей, которые не несут значимой информации для анализа. в рамках предварительной обработки было решено оставить следующие поля, которые считаются наиболее важными для дальнейшего анализа и выявления аномалий: коды ошибок (status codes): позволяют определить успешность или неудачу запроса и являются ключевыми для выявления проблем на веб-сервере. время запроса (timestamp): важно для анализа паттернов активности и идентификации нестандартного поведения, например, внезапных всплесков трафика. url (uniform resource locator): адрес запрашиваемого ресурса может указывать на целевые страницы атак или на необычные пути доступа. user-agent: информация о браузере и операционной системе пользователя помогает выявить подозрительные или устаревшие клиенты, потенциально уязвимые для эксплуатации. endpoint: конкретный ресурс на сервере, к которому был выполнен запрос, может выявить необычные или подозрительные пути доступа. size of response: размер ответа сервера может указывать на необычно большие или маленькие ответы, что также может быть признаком аномалии. выбор этих полей обусловлен тем, что статистическое отклонение по ним может служить индикатором аномального поведения. например, нестандартный размер ответа сервера или неожиданный всплеск в определенные периоды времени может указывать на попытки взлома, ddos-атаки или другие виды аномалий. анализ кодов ошибок может выявить не только технические проблемы, но и целенаправленные атаки на веб-сервер. таким образом, предварительная обработка данных заключается в исключении из рассмотрения нерелевантных полей и фокусировке внимания на тех атрибутах, которые могут предоставить наиболее ценную информацию для обнаружения и анализа аномальных событий. это создает основу для более эффективной и целенаправленной работы модели кластеризации и классификации событий.

  len(DocSection.structure): 3 DocSection.title: 3.3 описание алгоритма lvl: 3
  
    len(DocSection.structure): 0 DocSection.title: 3.3.1  dbscan lvl: 4
    так как основной задачей является разработка модели для кластеризации с выявлением аномалий, для достижения поставленной цели был выбран алгоритм dbscan, который является одним из наиболее подходящих методов для работы с данными такого типа. алгоритм dbscan начинает свою работу с выбора произвольной точки из набора данных и создания вокруг неё области с заданным радиусом, который называется эпсилон-окрестностью. если в этой окрестности обнаруживается количество точек, превышающее или равное минимально заданному порогу (minpts), то исходная точка классифицируется как корневая, что служит сигналом к формированию нового кластера. затем алгоритм переходит к поиску соседних точек. если в эпсилон-окрестности точки находится меньше, чем minpts соседей, но среди них есть хотя бы одна корневая точка, эта точка помечается как пограничная. пограничные точки не могут формировать новые кластеры, но могут быть частью уже существующих кластеров. все оставшиеся точки, которые не подпадают под критерии корневых или пограничных, считаются выбросами. они не включаются в кластеры и могут быть интерпретированы как аномалии или шум в данных. когда две корневые точки оказываются в пределах эпсилон-окрестности друг от друга, они объединяются в один кластер. это свойство позволяет кластерам формироваться и расти, поглощая новые корневые точки и их пограничные соседи. пограничные точки присоединяются к тем кластерам, к которым принадлежат корневые точки в их окрестности. таким образом, они служат своего рода мостом, связывающим близлежащие кластеры. процесс кластеризации завершается, когда все точки данных либо включены в кластеры, либо классифицированы как выбросы, и не остаётся ни одной точки, которую можно было бы добавить к существующим кластерам. это гарантирует, что каждая точка будет рассмотрена и получит своё место в общей структуре данных.

    len(DocSection.structure): 0 DocSection.title: 3.3.2  векторизация tf-idf lvl: 4
    для преобразования текстовых полей журналов в числовые векторы, которые могут быть использованы в кластеризации dbscan, применяется метод tf-idf. этот метод векторизации оценивает важность слова в контексте документа относительно всего корпуса документов. term frequency (tf) измеряет частоту слова в документе, а inverse document frequency (idf) уменьшает вес слов, которые встречаются очень часто и поэтому могут быть менее информативными. произведение tf и idf дает веса словам, которые помогают выделить наиболее значимые термины для каждого документа. векторизация tf-idf была реализована с использованием библиотеки scikit-learn[11], которая предоставляет инструменты для преобразования текстовых данных в формат, удобный для анализа. полученные векторы tf-idf затем использовались в качестве входных данных для алгоритма dbscan, позволяя модели эффективно кластеризовать записи журналов и выявлять аномалии на основе анализа текстового содержимого.

    len(DocSection.structure): 0 DocSection.title: 3.3.3  векторизация bert lvl: 4
    также использовалась векторизация текстовых полей журналов веб-серверов с помощью модели bert[12], которая была доработана на данных файлов журналов веб-серверов. в отличие от  tf-idf, данная модель использует нейронную сеть, состоящую из множества слоев, также учитывает расположение слова в текстовом поле и выдаёт разные векторы для одного и того же слова в зависимости от контекста. например для поля user-agent, модель bert могла выделить кластеры по платформам, которые использовали пользователи, а также находила поисковых роботов, являющихся составной частью поисковой системы yandex.



len(DocSection.structure): 2 DocSection.title: 4 анализ полученного решения lvl: 2

  len(DocSection.structure): 0 DocSection.title: 4.1 анализ решения, используя генерируемые данные lvl: 3
  использование сгенерированных данных для анализа решения предоставило начальное понимание работы модели кластеризации. эти данные были созданы таким образом, чтобы имитировать различные типы событий в журналах веб-серверов, включая нормальные запросы и потенциальные аномалии. однако при анализе результатов выяснилось, что модель, обученная и протестированная на таких данных, не обладает достаточной точностью разбиения на кластеры. при оценке точности кластеризации, проведенной на основе сгенерированных данных, столкнулись с субъективностью оценки. в отсутствие чётко определённых критериев и методов для точного выявления кластеров и аномалий, оценка эффективности модели не может быть полностью объективной. в частности, для генерируемых данных было замечено, что записи журналов веб-серверов группируются в 10 кластеров в зависимости от поля user-agent. учитывая, что в искусственном наборе данных присутствует только 10 уникальных значений user-agent, такое разделение не отражает реальную сложность и многообразие поведения пользователей веб-сервера. это наблюдение подтверждает, что кластеризация, основанная исключительно на ограниченном числе уникальных значений user-agent, не является оптимальной. такой подход может привести к излишней генерализации и игнорированию других важных аспектов данных, таких как временные паттерны, коды состояния http и размеры ответов, которые могут предоставить более глубокое понимание поведения системы и потенциальных угроз. ниже представлена визуализация в трехмерном пространстве кластерного анализа искусственных данных, которые разбиты на 10 кластеров(рис. 4) рисунок 4 — визуализация кластерного анализа искусственных данных

  len(DocSection.structure): 0 DocSection.title: 4.2 анализ решения, используя реальные данные lvl: 3
  переход к анализу решения на основе реальных данных журналов веб-серверов позволил значительно улучшить качество модели кластеризации. реальные данные обеспечили более точное и всестороннее представление о поведении системы, включая различные виды запросов и потенциальные угрозы. ниже представлена визуализация в трехмерном пространстве кластерного анализа реальных данных, которые разбиты уже на 20 кластеров(рис. 5) рисунок 5 — визуализация кластерного анализа реальных данных


len(DocSection.structure): 3 DocSection.title: 5 обеспечение качества разработки, продукции, программного продукта lvl: 2

  len(DocSection.structure): 0 DocSection.title: 5.1 определение потребителей lvl: 3
  потребителями разработанной системы могут быть: системные администраторы: используют систему для мониторинга и анализа событий веб-серверов, управления логами и выявления технических проблем, что помогает в обеспечении стабильности и доступности веб-сервисов. специалисты по информационной безопасности: применяют систему для обнаружения и реагирования на подозрительную активность и потенциальные угрозы безопасности, такие как вторжения или злоупотребления, что повышает уровень защиты данных и инфраструктуры. аналитики данных: используют систему для извлечения значимых инсайтов из данных журналов, таких как модели поведения пользователей и трафика, что способствует лучшему пониманию потребностей пользователей и оптимизации веб-контента. разработчики веб-приложений: интересуются системой как инструментом для отслеживания ошибок и недочетов в работе веб-приложений, а также для тестирования новых функций и изменений в коде. управляющий персонал it-департаментов: могут использовать систему для стратегического планирования и оптимизации ресурсов, а также для обеспечения соответствия требованиям нормативно-правовых актов по обработке и хранению данных. эти группы потребителей могут пользоваться системой для улучшения качества своей работы, повышения эффективности процессов, снижения рисков и обеспечения более высокого уровня удовлетворенности конечных пользователей. 5.2 функции продукции для того, чтобы ответить на вопрос, какие функции имеет разработанный продукт, необходимо ввести определение функции изделия или услуги. функции изделия или услуги – это требования и ожидания потребителя, которые могут быть установлены, предполагаются или являются обязательными. функциональность разработанного программного продукта должна соответствовать требованиям потребителей. для модели классификации событий это включает в себя: автоматическую обработку и анализ больших объемов данных журналов. высокую точность классификации событий для минимизации ложных срабатываний. гибкость в настройке параметров для адаптации к специфике различных веб-серверов. интеграцию с существующими системами мониторинга и управления. 5.3 качество и характеристики для обеспечения качества разрабатываемого курса был проведен обзор стандартов, протоколов, отраслевых требований и современных лучших практик, нужных в разработке. при этом качество – степень соответствия совокупности присущих характеристик объекта требованиям (согласно гост р исо 9000-2015). знание протоколов, стандартов и пр. позволяет разработчикам обеспечить соответствие курса современным требованиям, повысить эффективность и удобство использования, улучшить качество курса. анализ функциональных требований к разработке отражен в таблице (таблица 5). таблица 5 — функциональные тербования

  len(DocSection.structure): 0 DocSection.title: 5.4 измерение характеристик качества. операциональное определение lvl: 3
  операциональное определение (оо) – это уточнение значения того или иного термина применительно к данной системе, находящейся в конкретных условиях и для людей, в ней задействованных. оо необходим для уменьшения случаев разночтения и неоднозначности при общении между людьми, задействованными в системе, а также для более точного определения целей и задач, которые необходимо достичь. оо должно содержать как минимум три компоненты: требования или стандарт, относительно которого оценивается результат измерения или испытания (критерий). метод испытания или процедура измерения свойства объекта (тест) процедура принятия решения (анализ), которое показывает, соответствует ли результат испытания стандарту. операциональные определения для разработанного курса отражены в таблице (таблица 6). таблица 6 — операциональные определения. по итогам рассмотрения операциональных было определено, что разработанный курс удовлетворяет, описанным требованиям, однако существуют также предложения по улучшению продукта, которые сформулированы в таблице (таблица 7). таблица 7 — предложения по улучшению продукта.

  len(DocSection.structure): 0 DocSection.title: 5.5 выводы lvl: 3
  были определены потребители продукта, а также описаны ключевые функции конечного продукта. осуществлен обзор существующих стандартов, отраслевых требований и современных лучших практик, необходимых в разработке. по итогам обзора сформулированы функциональные требования к разработанной модели. описаны операциональные определения, характерные для модели классификации записей журналов веб-серверов и выявления аномалий. сделан вывод о том, что разработанная модель удовлетворяет описанным требованиям, а также сформулировано предложение по дальнейшему улучшению модели.


len(DocSection.structure): 0 DocSection.title: заключение lvl: 2
был проведен тщательный анализ существующих алгоритмов кластеризации, что позволило оценить их применимость для работы с журналами веб-серверов и выбрать наиболее эффективные в соответствии с задачами классификации событий. разработанный алгоритм предварительной обработки данных оказался неотъемлемой частью исследования, поскольку он значительно повысил качество и точность последующей классификации. обработка включала в себя очистку данных, нормализацию, а также выбор и преобразование признаков, что создало надежную основу для обучения модели. созданная модель классификации событий была обучена на реальных данных. она продемонстрировала способность  различать типы событий, а также выделять среди них аномалии. таким образом, результаты данной работы могут быть использованы для анализа событий журналов веб-серверов.

